[
["index.html", "Introduction to Model Selection Preface Outline", " Introduction to Model Selection Preface The purpose of this document is to summarise the concepts discussed in the model selection reading club – taking place in the Computational Systems Biology group at the D-BSSE (ETH Zurich) in Winter/Spring 2018. The reading club is split into two groups: One group covers the introduction into the topic and discusses the commonly used selection criteria Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC). The second group discusses their extensions for mixed-effects models. This document summerises the topics of both groups. The current structure of the document is shown below but might change during the course of the reading club. Outline Introduction The Bias-Variance Tradeoff Model Selection Criteria In general For Linear Mixed Models (LMEs) For Non-Linear Mixed Models (NLMEs) "],
["intro.html", "1 Introduction", " 1 Introduction The goal of model selection is to identify the best model among a set of competing models. What exactly means the best model? Usually, we consider the best model to be the simplest of the best fitting models, however, it is not exactly clear what the simplest or the best fitting model means. Consider for instance the following example: Figure 1.1: Example problem: which model is the best model? Here, \\(k\\) corresponds to the number of parameters (including the intercept). The sum over all squared residuals is given by \\(SSE\\) (sum of squared error): \\[SSE = \\sum_{i = 1}^n \\left( y(x_i) - \\hat{f}(x_i) \\right) ^ 2 = \\sum_{i = 1}^n \\underbrace{ \\left(\\underbrace{\\underbrace{y_i}_{\\text{observed}} - \\underbrace{\\hat{y}_i}_{\\text{predicted}}}_{\\text{residual } r_i}\\right)^2}_{\\text{squared error}}\\], where \\(n\\) corresponds to the number of observations (here \\(n = 20\\)), \\(y_i\\) is the observed response for given \\(x_i\\) and \\(\\hat{f}(x_i)\\) is the predicted reponse of model \\(\\hat{f}\\) for a given \\(x_i\\). Clearly, as more parameters the model has as better the model fits the data and as smaller is the sum of squared errors. But which of the three models is the best model? When does a better fit justifies more parameters or a more complex model? To answer this question, we first must decide on how to asses the quality of fit (is \\(SSE\\) the appropriate?) and how to measure model complexity. How to asses the quality of fit is the topic of the next chapter. Model complexity might be covered in another chapter. "],
["bias-var.html", "2 The Bias-Variance Tradeoff 2.1 The quality of fit in regression 2.2 Simulation study 2.3 Error decomposition", " 2 The Bias-Variance Tradeoff 2.1 The quality of fit in regression Quality of fit means usually how well our model \\(\\hat{f}\\) matches the experimental observations \\(y\\). Or more precisely, how much the predicted response value for a given observation \\(\\hat{f}(x_i) = \\hat{y}_i\\) is close to the observed response value \\(y(x_i)\\) for that observation \\(x_i\\). The \\(SSE\\) introduced in the previous section is not applicable because it is sample size dependent: as more observations we have, as higher the \\(SSE\\). Thus, we need to normalize for the number of observation \\(n\\). This gives us the most-commonly used measure for the quality of fit in regression analysis – the mean squared error \\(MSE\\): \\[MSE = \\dfrac{1}{n} \\sum_{i = 1}^n \\left( y(x_i) - \\hat{f}(x_i) \\right) ^ 2 = \\dfrac{1}{n} SSE \\ , \\] The MSE is the total mean error between our actual observations and model prediction. In the following section, we will see that there are different error sources that contribute to the total error (\\(SSE\\) and \\(MSE\\)). 2.2 Simulation study We perform a simulation study with the 4 example models introduced in the previous section @(intro). \\[\\begin{align} \\hat{f}_1 (x) &amp;= \\beta_0 + \\beta_1 \\ x &amp;\\text{(Linear model)} \\\\ \\hat{f}_2 (x) &amp;= \\beta_0 + \\beta_1 \\ x + \\beta_2 \\ x^2 &amp;\\text{(Quadratic model)} \\\\ \\hat{f}_3 (x) &amp;= \\beta_0 + \\beta_1 \\ x + \\beta_2 \\ x^2 + \\ldots + \\beta_{16} \\ x^{15} &amp; \\text{(Polynomial of degree 15)} \\\\ \\hat{f}_4 (x) &amp;= \\beta_0 + \\beta_1 \\ exp(x) &amp; \\text{(Exponential model)} \\\\ \\end{align}\\] All models share the same vector of independent variables \\(x\\). The parameter spaces of \\(\\hat{f}_1\\) and \\(\\hat{f}_4\\) are strict subsets of the parameter space of \\(\\hat{f}_2\\); and the parameter space of \\(\\hat{f}_2\\) is a strict subset of \\(\\hat{f}_3\\). But while model \\(\\hat{f}_{3}\\) can include models \\(\\hat{f}_{i&lt;3}\\) (and \\(\\hat{f}_{2}\\) can include \\(\\hat{f}_{1}\\)), model \\(\\hat{f}_4\\) belongs to a different category. We will see why this is important. Assume you have a set of observations \\(y\\) from given by some data-generating process for given values of \\(x\\). For instance, \\(x\\) might be a set of experimental conditions or a set of patient biomarkers. Then \\(y\\) might correspond to a fluorescence signal under the condition \\(x\\) or a disease outcome in the considered patient population. Note, that you do not observe the full population from which \\(y\\) is drawn but only a sample; performing another experiment under conditions \\(x\\) or collecting more patient samples for the considered patient population corresponds ideally to taking a new random sample from the same population. We try to design our experiments and studies such that we can assume that this is the case, otherwise we cannot use the discussed model selection criteria, because then we cannot assume that the observations come from the same data-generating process and thus can be explained by the same model \\(\\hat{f}\\). We assume there is some real unknown model \\(f\\) that gives rise to our observations \\(y\\): \\[ y = f(x) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2) \\] If we fit the four models to our available obseravtions \\(y\\), we get the following quality of fit: get.statinfo &lt;- function(model, model.title, idx) { tibble( Model = model.title[idx], k = length(model[[idx]]$coefficients), n = nobs(model[[idx]]), df = df.residual(model[[idx]]), # n - k SSE = round(sum(model[[idx]]$residuals ^ 2), 0), MSE = round((sum(model[[idx]]$residuals ^ 2)) / nobs(model[[idx]]), 0) ) } statinfo &lt;- purrr::map( seq(4), .f = function(.) get.statinfo(model, model.title, .) ) %&gt;% do.call(&quot;rbind&quot;, .) knitr::kable( statinfo, caption = &#39;Summary statistics for fitting the four example models to the training data.&#39;, booktabs = TRUE ) Table 2.1: Summary statistics for fitting the four example models to the training data. Model k n df SSE MSE Linear model 2 20 18 89005 4450 Quadratic model 3 20 17 26197 1310 Polynomial of degree 15 16 20 4 8700 435 Exponential model 2 20 18 638936 31947 The \\(MSE\\) between model and training data is often called training \\(MSE\\). But to asses the model performance we are usually not interested in the training \\(MSE\\). Instead, we are interested in how well our model performs on new observations – the test \\(MSE\\). Let us compute the test \\(MSE\\) of the four example models for 1000 new observations, i.e. 1000 new samples drawn from the same population of observation as the training data set \\(y\\). .compute.test.MSE &lt;- function(model, seed, idx) { set.seed(seed) test.data &lt;- tibble::tibble(x = seq(20), y = jitter(2 * x ^ 2, amount = 60)) pred.y &lt;- c(predict.lm(model[[idx]], test.data)) test.MSE &lt;- mean((test.data$y - pred.y) ^ 2) return(test.MSE) } n.obs &lt;- 1000 # number of new observations test.MSE &lt;- purrr::map( seq(4), .f = function(.) lapply(seq(n.obs), function(x) .compute.test.MSE(model, x, .)) %&gt;% do.call(&quot;rbind&quot;, .) ) plot.test.MSE &lt;- function(test.MSE, model.title, idx) { ggplot2::ggplot() + geom_histogram( aes(x = test.MSE[[idx]]), color = &quot;black&quot;, fill = &quot;white&quot;, bins = 20) + theme_book() + labs(title = model.title[idx], x = &quot;Test MSE&quot;, y = &quot;Frequency&quot;) + geom_vline( xintercept = median(test.MSE[[idx]]), lty = 3, color = &quot;red&quot;, lwd = 1.2) } hist.MSE &lt;- purrr::map( seq(4), .f = function(.) plot.test.MSE(test.MSE, model.title, .) ) invisible(lapply(hist.MSE, function(x) show(x))) # TODO: fit gaussian and estimate sigma**2 Figure 2.1: Comparison of test MSEs for the three example models; computed on 1000 new data sets. The dashed red line indicates the median. Figure 2.2: Training MSE (triangles) and test MSE (circles) for the four example models. The polynomial of degree 15 has the best training \\(MSE\\) but the worse test \\(MSE\\). This behavior is often called overfitting. In the next section we will show that this results from high model variance (error due to estimation) and low bias (error due to approximation). The linear model performs bad on both data sets. Note that the \\(MSE\\) of the linear model does not change as as much as for \\(\\hat{f}_3\\) – this indicates that the model variance of \\(\\hat{f}_1\\) is rather low and that the high \\(MSE\\) results from high bias. The quadratic model performs well on both data sets suggesting a low model variance and a low bias. And indeed, this model was used to generate the data for this example. 2.3 Error decomposition [work in progress] Assume we know the real model \\(f\\) that gives rise to some real observations \\(y\\) for values \\(x\\) described by \\(y = f(x) + \\epsilon\\), with random noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). We are interested in the quality of fit for a given model \\(\\hat{f}\\). It can be shown, that the expected test \\(MSE\\) for \\(\\hat{f}(x_i)\\) can always be decomposed into the sum of the following three components: \\[ E \\left( y(x_i) - \\hat{f}(x_i) \\right)^2 = \\mathrm{Var}(\\hat{f}(x_i)) + \\left( \\mathrm{Bias} (\\hat{f}(x_i)) \\right)^2 + \\mathrm{Var}(\\epsilon)\\] Variance of the fitted model: \\(\\mathrm{Var}(\\hat{f}(x_i)) = E(\\hat{f}(x_i)^2) - E(\\hat{f}(x_i))^2\\) The variance of \\(\\hat{f}(x_i)\\) refers to the amount by which model \\(\\hat{f}\\) would change, if we estimated it using a different training data set (changes in estimated parameters); in general, more flexible models can result in large changes in \\(\\hat{f}\\). Figure 2.3: Visualization of model variance: The example models were fitted to four different testing data sets. Squared bias of the fitted model: \\(\\left( \\mathrm{Bias} (\\hat{f}(x_i)) \\right)^2 = \\left( E\\left(\\hat{f}(x_i)\\right) - f(x_i) \\right)^2\\) The squared bias of \\(\\hat{f}(x_i)\\). Variance due to measurement noise: \\(\\mathrm{Var}(\\epsilon) = \\sigma^2\\) The variance of the error terms, which is irreducible. "]
]
