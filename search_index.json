[
["index.html", "Introduction to Model Selection Preface Outline", " Introduction to Model Selection Last update: March 13, 2018 Preface The purpose of this document is to summarise the concepts discussed in the model selection reading club – taking place in the Computational Systems Biology group at the D-BSSE (ETH Zurich) in Winter/Spring 2018. The reading club is split into two groups: One group covers the introduction into the topic and discusses the commonly used selection criteria Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC). The second group discusses their extensions for mixed-effects models. This document summerises the topics of both groups. The current structure of the document is shown below but might change during the course of the reading club. Outline Introduction The Bias-Variance Tradeoff Model Selection Criteria In general For Linear Mixed Models (LMEs) For Non-Linear Mixed Models (NLMEs) "],
["introduction.html", "1 Introduction", " 1 Introduction The goal of model selection is to identify the best model among a set of competing models. What exactly means the best model? Usually, we consider the best model to be the simplest of the best fitting models, however, it is not exactly clear what the simplest or the best fitting model means. Consider for instance the following example: Figure 1.1: Example problem: which model is the best model? Here, \\(k\\) corresponds to the number of parameters (including the intercept) and \\(SSE\\) is the sum of squared error given by \\[SSE = \\sum_{i = 1}^n (y_i - f(x_i))^2 ,\\] where \\(n\\) corresponds to the number of observations (here \\(n = 20\\)), \\(y_i\\) is the \\(i\\)th observation and \\(f(x_i)\\) is the predicted reponse of model \\(f\\) for observation \\(i\\). We observe that the \\(SSE\\) significantly decreases with increasing \\(k\\). So which of the three models is the best model? When does a better fit justifies a more complex model? To answer this question, we must decide on how to asses the quality of fit and how to measure model complexity. How to measure quality of fit is the topic of the next chapter. Model complexity might be covered in another section. "],
["bias-var.html", "2 The Bias-Variance Tradeoff 2.1 Mean squared error 2.2 Simulation study 2.3 Error decomposition", " 2 The Bias-Variance Tradeoff 2.1 Mean squared error Quality of fit means usually how well our model matches the experimental observations. Or more precisely, how much the predicted response value for a given observation is close to the true response value for that observation. The \\(SSE\\) introduced in the previous section is not applicable as it is sample size dependent. The most-commonly used measure in regression is the mean squared error (\\(MSE\\)), given by: \\[MSE = \\dfrac{1}{n} \\sum_{i = 1}^n (y_i - f(x_i))^2 ,\\] 2.2 Simulation study We consider the three models given in the introduction. We can compute the \\(MSE\\) for the given data in the introduction. get.statinfo &lt;- function(m, title, idx) { tibble( model = title[idx], k = length(m[[idx]]$coefficients), n = nobs(m[[idx]]), df = df.residual(m[[idx]]), # n - k SSE = round(sum(m[[idx]]$residuals ^ 2), 0), MSE = round((sum(m[[idx]]$residuals ^ 2)) / nobs(m[[idx]]), 0) ) } statinfo &lt;- purrr::map(seq(3), .f = function(.) get.statinfo(model, title, .)) %&gt;% do.call(&quot;rbind&quot;, .) knitr::kable( statinfo, caption = &#39;Summary statistics for the three example models and the training data.&#39;, booktabs = TRUE ) (#tab:02-sum-stat_tab)Summary statistics for the three example models and the training data. model k n df SSE MSE Linear model 2 20 18 112222 5611 Quadratic model 3 20 17 22091 1105 Polynomial of degree 15 16 20 4 8456 423 However, to asses the model performance, we are not interested in the quality of fit of the training data, i. e. the training \\(MSE\\). Instead, we are interested in how well our model performs on new observations – the test \\(MSE\\). Let us compute the test \\(MSE\\) of the three example models for 1000 new observations: .compute.test.MSE &lt;- function(m, seed, idx){ set.seed(seed) test.df &lt;- tibble::tibble(x = seq(20), y = jitter(2 * x ^ 2, amount = 60)) pred.y &lt;- c(predict.lm(m[[idx]], test.df)) test.MSE &lt;- mean((test.df$y - pred.y) ^ 2) return(test.MSE) } test.MSEs &lt;- purrr::map( seq(3), .f = function(.) lapply(seq(1000), function(x) .compute.test.MSE(model, x, .)) %&gt;% do.call(&quot;rbind&quot;, .) ) plot.hist &lt;- function(test.MSEs, title, idx){ ggplot2::ggplot() + geom_histogram(aes(x = test.MSEs[[idx]]), color = &#39;black&#39;, fill = &#39;white&#39;, bins = 20) + theme_book() + labs(title = title[idx], x = &quot;Test MSE&quot;, y = &quot;Count&quot;) + geom_vline(xintercept = median(test.MSE[[idx]]), lty = 3, color = &#39;red&#39;, lwd = 1.2) } hist.MSE &lt;- purrr::map( seq(3), .f = function(.) plot.hist(test.MSEs, title, .) ) invisible(lapply(hist.MSE, function(x) show(x))) Figure 2.1: Comparison of test MSEs for the three example models; computed on 1000 new data sets. As expected, while the polynomial of degree 15 has the best training \\(MSE\\), it has the worse test \\(MSE\\) – which means this model is overfitting the data. The quadratic model (which we know is the true model) shows almost the same training and test \\(MSE\\). Figure 2.2: Training MSE (triangles) and test MSE (circles) for the three example models. 2.3 Error decomposition Assume we know the real model \\(f\\) that gives rise to some real observations \\(y\\) for values \\(x\\) described by \\(y = f(x) + \\epsilon\\), with random noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). We are interested in the quality of fit for a given model \\(\\hat{f}\\). It can be shown, that the expected test \\(MSE\\) for \\(\\hat{f}(x_i)\\) can always be decomposed into the sum of the following three components: \\[ E \\left( f(x_i) - \\hat{f}(x_i) \\right)^2 = \\mathrm{Var}(\\hat{f}(x_i)) + \\left( \\mathrm{Bias} (\\hat{f}(x_i)) \\right)^2 + \\mathrm{Var}(\\epsilon)\\] Variance of the fitted model: \\(\\mathrm{Var}(\\hat{f}(x_i)) = E(\\hat{f}(x_i)^2) - E(\\hat{f}(x_i))^2\\) The variance of \\(\\hat{f}(x_i)\\) refers to the amount by which model \\(\\hat{f}\\) would change, if we estimated it using a different training data set (changes in estimated parameters); in general, more flexible models can result in large changes in \\(\\hat{f}\\). Figure 2.3: Visualization of model variance: The example models were fitted to four different testing data sets. Squared bias of the fitted model: \\(\\left( \\mathrm{Bias} (\\hat{f}(x_i)) \\right)^2 = \\left( E\\left(\\hat{f}(x_i)\\right) - f(x_i) \\right)^2\\) The squared bias of \\(\\hat{f}(x_i)\\). Variance due to measurement noise: \\(\\mathrm{Var}(\\epsilon) = \\sigma^2\\) The variance of the error terms, which is irreducible. "]
]
